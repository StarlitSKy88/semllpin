groups:
  # Critical Service Availability Alerts
  - name: service_availability
    interval: 30s
    rules:
      - alert: ServiceDown
        expr: smellpin_endpoint_availability{critical="true"} == 0
        for: 2m
        labels:
          severity: critical
          team: devops
          category: availability
        annotations:
          title: "Critical Service Down"
          description: "{{ $labels.service }} endpoint {{ $labels.endpoint }} has been down for more than 2 minutes"
          runbook_url: "https://runbooks.smellpin.com/alerts/service-down"
          dashboard: "https://monitoring.smellpin.com/d/service-overview"

      - alert: ServiceDegraded
        expr: |
          (
            rate(smellpin_response_time_seconds_sum[5m]) / 
            rate(smellpin_response_time_seconds_count[5m])
          ) > 0.5
        for: 5m
        labels:
          severity: warning
          team: devops
          category: performance
        annotations:
          title: "Service Performance Degraded"
          description: "{{ $labels.service }} response time is above 500ms for 5 minutes"
          current_value: "{{ $value }}s"

      - alert: HighErrorRate
        expr: |
          (
            rate(smellpin_http_errors_total[5m]) / 
            rate(smellpin_response_time_seconds_count[5m])
          ) * 100 > 1
        for: 3m
        labels:
          severity: warning
          team: devops
          category: errors
        annotations:
          title: "High Error Rate Detected"
          description: "{{ $labels.service }} error rate is {{ $value | humanizePercentage }} for 3 minutes"

  # SLA Compliance Alerts
  - name: sla_compliance
    interval: 60s
    rules:
      - alert: UptimeSLAViolation
        expr: smellpin_service_uptime_percentage{window="30d"} < 99.9
        for: 0s
        labels:
          severity: critical
          team: devops
          category: sla
        annotations:
          title: "99.9% Uptime SLA Violation"
          description: "{{ $labels.service }} uptime is {{ $value | humanizePercentage }} over 30 days, below 99.9% SLA"

      - alert: ResponseTimeSLAViolation
        expr: |
          histogram_quantile(0.95, 
            rate(smellpin_response_time_seconds_bucket[1h])
          ) > 0.2
        for: 15m
        labels:
          severity: warning
          team: devops
          category: sla
        annotations:
          title: "Response Time SLA Violation"
          description: "{{ $labels.service }} P95 response time is {{ $value }}s, above 200ms SLA"

      - alert: ErrorRateSLAViolation
        expr: smellpin_error_rate_percentage > 0.1
        for: 10m
        labels:
          severity: warning
          team: devops
          category: sla
        annotations:
          title: "Error Rate SLA Violation"
          description: "{{ $labels.service }} error rate is {{ $value }}%, above 0.1% SLA"

  # Infrastructure Alerts
  - name: infrastructure
    interval: 30s
    rules:
      - alert: HighCPUUsage
        expr: (100 - (avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 80
        for: 10m
        labels:
          severity: warning
          team: infrastructure
          category: resources
        annotations:
          title: "High CPU Usage"
          description: "CPU usage is {{ $value | humanizePercentage }} on {{ $labels.instance }}"

      - alert: HighMemoryUsage
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 85
        for: 10m
        labels:
          severity: warning
          team: infrastructure
          category: resources
        annotations:
          title: "High Memory Usage"
          description: "Memory usage is {{ $value | humanizePercentage }} on {{ $labels.instance }}"

      - alert: DiskSpaceRunningOut
        expr: (node_filesystem_free_bytes{fstype!="tmpfs"} / node_filesystem_size_bytes{fstype!="tmpfs"}) * 100 < 15
        for: 5m
        labels:
          severity: critical
          team: infrastructure
          category: storage
        annotations:
          title: "Disk Space Running Out"
          description: "Disk space is {{ $value | humanizePercentage }} free on {{ $labels.instance }}"

      - alert: ContainerDown
        expr: up{job="cadvisor"} == 0
        for: 2m
        labels:
          severity: warning
          team: devops
          category: containers
        annotations:
          title: "Container Monitor Down"
          description: "cAdvisor is down on {{ $labels.instance }}"

  # Application-Specific Alerts
  - name: application_health
    interval: 30s
    rules:
      - alert: DatabaseConnectionIssues
        expr: smellpin_endpoint_availability{service="infrastructure", endpoint=~".*db.*"} == 0
        for: 1m
        labels:
          severity: critical
          team: backend
          category: database
        annotations:
          title: "Database Connection Issues"
          description: "Database health check failing for {{ $labels.service }}"

      - alert: RedisConnectionIssues
        expr: redis_up == 0
        for: 2m
        labels:
          severity: warning
          team: backend
          category: cache
        annotations:
          title: "Redis Connection Issues"
          description: "Redis is down on {{ $labels.instance }}"

      - alert: PaymentServiceDown
        expr: smellpin_endpoint_availability{service="api", endpoint=~".*payment.*"} == 0
        for: 30s
        labels:
          severity: critical
          team: backend
          category: payments
        annotations:
          title: "Payment Service Critical"
          description: "Payment service is down - immediate revenue impact"

      - alert: LBSServiceDown
        expr: smellpin_endpoint_availability{service="api", endpoint=~".*lbs.*"} == 0
        for: 1m
        labels:
          severity: critical
          team: backend
          category: core
        annotations:
          title: "LBS Service Down"
          description: "Location-based service is down - core functionality affected"

  # Security and External Dependencies
  - name: security_and_external
    interval: 60s
    rules:
      - alert: SSLCertificateExpiry
        expr: (probe_ssl_earliest_cert_expiry - time()) / 86400 < 30
        for: 0s
        labels:
          severity: warning
          team: devops
          category: security
        annotations:
          title: "SSL Certificate Expiring Soon"
          description: "SSL certificate for {{ $labels.instance }} expires in {{ $value }} days"

      - alert: ExternalServiceDown
        expr: probe_success{job="blackbox-http"} == 0
        for: 2m
        labels:
          severity: warning
          team: devops
          category: external
        annotations:
          title: "External Dependency Down"
          description: "External service {{ $labels.instance }} is unreachable"

  # Business Impact Alerts
  - name: business_impact
    interval: 300s
    rules:
      - alert: HighUserImpactIncident
        expr: smellpin_users_impacted > 5000
        for: 0s
        labels:
          severity: critical
          team: oncall
          category: business
        annotations:
          title: "High User Impact Incident"
          description: "Incident {{ $labels.incident_id }} affecting {{ $value }} users"

      - alert: LongRunningIncident
        expr: time() - smellpin_incident_start_time > 1800
        for: 0s
        labels:
          severity: critical
          team: oncall
          category: incident
        annotations:
          title: "Long Running Incident"
          description: "Incident {{ $labels.incident_id }} has been open for over 30 minutes"

      - alert: MTTRExceeded
        expr: smellpin_mttr_seconds > 900
        for: 0s
        labels:
          severity: warning
          team: devops
          category: sla
        annotations:
          title: "MTTR SLA Exceeded"
          description: "Mean Time To Recovery is {{ $value }}s, exceeding 15-minute target"

  # Monitoring System Health
  - name: monitoring_health
    interval: 30s
    rules:
      - alert: PrometheusDown
        expr: up{job="prometheus"} == 0
        for: 30s
        labels:
          severity: critical
          team: devops
          category: monitoring
        annotations:
          title: "Prometheus Down"
          description: "Prometheus monitoring system is down"

      - alert: AlertmanagerDown
        expr: up{job="alertmanager"} == 0
        for: 1m
        labels:
          severity: critical
          team: devops
          category: monitoring
        annotations:
          title: "Alertmanager Down"
          description: "Alertmanager is down - alerts may not be delivered"

      - alert: MonitoringDataLoss
        expr: increase(prometheus_tsdb_data_replay_duration_seconds[1h]) > 0
        for: 0s
        labels:
          severity: warning
          team: devops
          category: monitoring
        annotations:
          title: "Monitoring Data Replay Detected"
          description: "Prometheus has replayed data, possible data loss occurred"