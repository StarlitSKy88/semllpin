version: '3.8'

# SmellPin Production Environment
# Auto-scaling, load balancing, and enterprise-grade deployment

services:
  # Load Balancer - HAProxy with SSL termination
  loadbalancer:
    image: haproxy:2.8-alpine
    container_name: smellpin-loadbalancer
    ports:
      - "80:80"
      - "443:443"
      - "8404:8404"  # Stats interface
    volumes:
      - ./config/haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
      - ./security/ssl:/etc/ssl/certs:ro
      - ./scripts:/scripts:ro
    environment:
      - STATS_USER=${HAPROXY_STATS_USER:-admin}
      - STATS_PASS=${HAPROXY_STATS_PASS:-SmellPin2024!}
      - SSL_CERT_PATH=/etc/ssl/certs
    networks:
      - frontend-network
      - backend-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8404/stats"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    deploy:
      placement:
        constraints:
          - node.role == manager
      replicas: 2
      update_config:
        parallelism: 1
        delay: 10s

  # Frontend Services (Auto-scaling)
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.production
    environment:
      - NODE_ENV=production
      - VITE_API_URL=https://api.smellpin.com
      - VITE_STRIPE_PUBLISHABLE_KEY=${VITE_STRIPE_PUBLISHABLE_KEY}
      - NEXT_PUBLIC_MAPBOX_ACCESS_TOKEN=${NEXT_PUBLIC_MAPBOX_ACCESS_TOKEN}
      - NEXT_PUBLIC_OSM_TILE_URL=${NEXT_PUBLIC_OSM_TILE_URL}
      - NEXT_PUBLIC_NOMINATIM_URL=${NEXT_PUBLIC_NOMINATIM_URL}
      - VITE_ANALYTICS_ID=${VITE_ANALYTICS_ID}
    networks:
      - frontend-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    deploy:
      replicas: 3
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
      resources:
        limits:
          memory: 1G
          cpus: '0.8'
        reservations:
          memory: 512M
          cpus: '0.4'
    logging:
      driver: json-file
      options:
        max-size: "100m"
        max-file: "5"

  # Backend API Services (Auto-scaling)
  api:
    build:
      context: .
      dockerfile: Dockerfile.production
      target: production
    environment:
      - NODE_ENV=production
      - PORT=3000
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - JWT_SECRET=${JWT_SECRET}
      - JWT_EXPIRES_IN=${JWT_EXPIRES_IN:-24h}
      - STRIPE_SECRET_KEY=${STRIPE_SECRET_KEY}
      - PAYPAL_CLIENT_ID=${PAYPAL_CLIENT_ID}
      - PAYPAL_CLIENT_SECRET=${PAYPAL_CLIENT_SECRET}
      - MAPBOX_ACCESS_TOKEN=${MAPBOX_ACCESS_TOKEN}
      - OSM_TILE_SERVER_API_KEY=${OSM_TILE_SERVER_API_KEY}
      - AMAP_KEY=${AMAP_KEY}
      - BAIDU_MAP_AK=${BAIDU_MAP_AK}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION:-us-east-1}
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
      - SMTP_HOST=${SMTP_HOST}
      - SMTP_PORT=${SMTP_PORT}
      - SMTP_USER=${SMTP_USER}
      - SMTP_PASS=${SMTP_PASS}
      - MAX_REQUEST_SIZE=${MAX_REQUEST_SIZE:-10mb}
      - RATE_LIMIT_WINDOW=${RATE_LIMIT_WINDOW:-15}
      - RATE_LIMIT_MAX=${RATE_LIMIT_MAX:-100}
    volumes:
      - api-uploads:/app/uploads
      - api-logs:/app/logs
    networks:
      - backend-network
      - database-network
    depends_on:
      - postgres
      - redis
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    deploy:
      replicas: 4
      update_config:
        parallelism: 2
        delay: 10s
        failure_action: rollback
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
      resources:
        limits:
          memory: 2G
          cpus: '1.5'
        reservations:
          memory: 1G
          cpus: '0.75'
    logging:
      driver: json-file
      options:
        max-size: "200m"
        max-file: "10"

  # PostgreSQL Database with replication
  postgres:
    image: postgres:15-alpine
    container_name: smellpin-postgres-primary
    environment:
      - POSTGRES_DB=${POSTGRES_DB:-smellpin}
      - POSTGRES_USER=${POSTGRES_USER:-smellpin}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_INITDB_ARGS=--auth-host=md5
      - POSTGRES_HOST_AUTH_METHOD=md5
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./database/init:/docker-entrypoint-initdb.d:ro
      - ./database/config/postgresql.conf:/etc/postgresql/postgresql.conf:ro
      - ./database/config/pg_hba.conf:/etc/postgresql/pg_hba.conf:ro
    networks:
      - database-network
    ports:
      - "5432:5432"
    command: [
      "postgres",
      "-c", "config_file=/etc/postgresql/postgresql.conf",
      "-c", "hba_file=/etc/postgresql/pg_hba.conf"
    ]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-smellpin} -d ${POSTGRES_DB:-smellpin}"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    deploy:
      placement:
        constraints:
          - node.labels.database == true
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    logging:
      driver: json-file
      options:
        max-size: "100m"
        max-file: "5"

  # PostgreSQL Read Replica
  postgres-replica:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=${POSTGRES_DB:-smellpin}
      - POSTGRES_USER=${POSTGRES_USER:-smellpin}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_PRIMARY_HOST=postgres
      - POSTGRES_REPLICA_MODE=replica
    volumes:
      - postgres-replica-data:/var/lib/postgresql/data
      - ./database/replica:/docker-entrypoint-initdb.d:ro
    networks:
      - database-network
    ports:
      - "5433:5432"
    depends_on:
      - postgres
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-smellpin} -d ${POSTGRES_DB:-smellpin}"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    deploy:
      placement:
        constraints:
          - node.labels.database == true
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'

  # Redis Cluster
  redis:
    image: redis:7-alpine
    container_name: smellpin-redis-primary
    environment:
      - REDIS_REPLICATION_MODE=master
    volumes:
      - redis-data:/data
      - ./database/redis/redis.conf:/usr/local/etc/redis/redis.conf:ro
    networks:
      - database-network
    ports:
      - "6379:6379"
    command: redis-server /usr/local/etc/redis/redis.conf
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    deploy:
      placement:
        constraints:
          - node.labels.redis == true
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'

  # Redis Sentinel for high availability
  redis-sentinel:
    image: redis:7-alpine
    environment:
      - REDIS_SENTINEL_MODE=yes
      - REDIS_MASTER_NAME=smellpin-master
      - REDIS_MASTER_HOST=redis
      - REDIS_MASTER_PORT=6379
    volumes:
      - ./database/redis/sentinel.conf:/usr/local/etc/redis/sentinel.conf:ro
    networks:
      - database-network
    ports:
      - "26379:26379"
    command: redis-sentinel /usr/local/etc/redis/sentinel.conf
    depends_on:
      - redis
    restart: unless-stopped
    deploy:
      replicas: 3
      resources:
        limits:
          memory: 256M
          cpus: '0.2'

  # Auto-scaling Controller
  autoscaler:
    build:
      context: ./autoscaler
      dockerfile: Dockerfile
    container_name: smellpin-autoscaler
    environment:
      - NODE_ENV=production
      - DOCKER_HOST=unix:///var/run/docker.sock
      - PROMETHEUS_URL=http://prometheus:9090
      - SCALING_RULES_CONFIG=/app/config/scaling-rules.json
      - MIN_REPLICAS_API=2
      - MAX_REPLICAS_API=10
      - MIN_REPLICAS_FRONTEND=2
      - MAX_REPLICAS_FRONTEND=8
      - CPU_TARGET=70
      - MEMORY_TARGET=80
      - RESPONSE_TIME_TARGET=200
      - SCALING_COOLDOWN=300
      - SLACK_WEBHOOK_URL=${SLACK_WEBHOOK_URL}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./autoscaler/config:/app/config:ro
      - autoscaler-logs:/app/logs
    networks:
      - monitoring-network
      - backend-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3003/health"]
      interval: 60s
      timeout: 15s
      retries: 3
    restart: unless-stopped
    deploy:
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          memory: 512M
          cpus: '0.3'

  # Circuit Breaker Service
  circuit-breaker:
    build:
      context: ./circuit-breaker
      dockerfile: Dockerfile
    container_name: smellpin-circuit-breaker
    environment:
      - NODE_ENV=production
      - CIRCUIT_BREAKER_PORT=3004
      - FAILURE_THRESHOLD=5
      - RECOVERY_TIMEOUT=30000
      - MONITOR_INTERVAL=10000
      - SERVICES_CONFIG=/app/config/services.json
    volumes:
      - ./circuit-breaker/config:/app/config:ro
      - circuit-breaker-logs:/app/logs
    networks:
      - backend-network
      - monitoring-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3004/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 256M
          cpus: '0.2'

  # Prometheus for metrics
  prometheus:
    image: prom/prometheus:v2.47.0
    container_name: smellpin-prometheus-prod
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus/prometheus-prod.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/prometheus/rules:/etc/prometheus/rules:ro
      - prometheus-prod-data:/prometheus
    networks:
      - monitoring-network
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--storage.tsdb.retention.size=10GB'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    deploy:
      placement:
        constraints:
          - node.labels.monitoring == true
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'

  # Grafana for visualization
  grafana:
    image: grafana/grafana:10.1.0
    container_name: smellpin-grafana-prod
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=https://monitoring.smellpin.com
      - GF_SMTP_ENABLED=true
      - GF_SMTP_HOST=${SMTP_HOST}:${SMTP_PORT}
      - GF_SMTP_USER=${SMTP_USER}
      - GF_SMTP_PASSWORD=${SMTP_PASS}
      - GF_SMTP_FROM_ADDRESS=monitoring@smellpin.com
    volumes:
      - grafana-prod-data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    networks:
      - monitoring-network
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    deploy:
      placement:
        constraints:
          - node.labels.monitoring == true
      resources:
        limits:
          memory: 1G
          cpus: '0.5'

  # Log Aggregation
  loki:
    image: grafana/loki:2.9.0
    container_name: smellpin-loki-prod
    ports:
      - "3100:3100"
    volumes:
      - ./monitoring/loki/loki-prod.yml:/etc/loki/local-config.yaml:ro
      - loki-prod-data:/loki
    networks:
      - monitoring-network
    command: -config.file=/etc/loki/local-config.yaml
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'

  # Health Check Service
  healthcheck:
    build:
      context: ./healthcheck
      dockerfile: Dockerfile
    container_name: smellpin-healthcheck
    environment:
      - NODE_ENV=production
      - CHECK_INTERVAL=15000
      - HEALTH_ENDPOINT_TIMEOUT=10000
      - ALERT_COOLDOWN=300000
      - SLACK_WEBHOOK_URL=${SLACK_WEBHOOK_URL}
      - PAGERDUTY_INTEGRATION_KEY=${PAGERDUTY_INTEGRATION_KEY}
    volumes:
      - ./healthcheck/config/endpoints.json:/app/config/endpoints.json:ro
      - healthcheck-logs:/app/logs
    networks:
      - backend-network
      - monitoring-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3005/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 256M
          cpus: '0.2'

volumes:
  # Persistent data volumes
  postgres-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/smellpin/production/data/postgres

  postgres-replica-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/smellpin/production/data/postgres-replica

  redis-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/smellpin/production/data/redis

  prometheus-prod-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/smellpin/production/data/prometheus

  grafana-prod-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/smellpin/production/data/grafana

  loki-prod-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/smellpin/production/data/loki

  # Application data volumes
  api-uploads:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/smellpin/production/uploads

  # Log volumes
  api-logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/smellpin/production/logs/api

  autoscaler-logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/smellpin/production/logs/autoscaler

  circuit-breaker-logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/smellpin/production/logs/circuit-breaker

  healthcheck-logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/smellpin/production/logs/healthcheck

networks:
  # Multi-tier network architecture
  frontend-network:
    driver: overlay
    attachable: true
    labels:
      - "tier=frontend"

  backend-network:
    driver: overlay
    attachable: true
    labels:
      - "tier=backend"

  database-network:
    driver: overlay
    internal: true
    labels:
      - "tier=database"

  monitoring-network:
    driver: overlay
    attachable: true
    labels:
      - "tier=monitoring"

configs:
  haproxy_config:
    file: ./config/haproxy/haproxy.cfg
  nginx_config:
    file: ./security/nginx-security.conf
  postgres_config:
    file: ./database/config/postgresql.conf

secrets:
  postgres_password:
    external: true
  jwt_secret:
    external: true
  stripe_secret_key:
    external: true
  grafana_admin_password:
    external: true